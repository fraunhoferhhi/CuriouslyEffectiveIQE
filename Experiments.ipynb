{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook describes the computations to reproduce the correlations from tables 1 and 2.\n",
    "\n",
    "There are 6 steps:\n",
    "\n",
    "(Run the following 2 steps outside of this notebook)\n",
    "\n",
    "1. Download and format the datasets.\n",
    "- Run the shell script *feature_extraction.sh* from your terminal.\n",
    "\n",
    "(The following 5 steps are implemented in this notebook)\n",
    "\n",
    "3. Load features as extracted with *feature_extraction.sh*\n",
    "- Training an SVM on the features extracted from the LIVE database. We repeat this 10 times per codebook model, every time using a different random train/test split.\n",
    "- Cross-database evaluation: We load the trained SVMs and evaluate them on the features extracted from TID2013 and CSIQ.\n",
    "- Computation of correlations: Correlations are computed per train/test split and then averaged across splits. We compute correlations on the full datasets as well as on distortion specific subsets.\n",
    "- Print results tables (Tables 1 and 2 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from joblib import dump, load\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of reference images in validation set for training/validating.\n",
    "# Every row corresponds to one split.\n",
    "val_names = [['buildings', 'statue', 'woman', 'monarch', 'paintedhouse','lighthouse2'],\n",
    "             ['ocean', 'sailing3', 'caps', 'lighthouse', 'bikes', 'studentsculpture'],\n",
    "             ['monarch', 'studentsculpture', 'parrots', 'stream', 'sailing3', 'sailing1'],\n",
    "             ['coinsinfountain', 'manfishing', 'rapids', 'cemetry', 'building2', 'monarch'],\n",
    "             ['parrots', 'buildings', 'woman', 'dancers', 'sailing3', 'carnivaldolls'],\n",
    "             ['lighthouse2', 'building2', 'stream', 'ocean', 'woman', 'rapids'],\n",
    "             ['sailing2', 'lighthouse2', 'parrots', 'manfishing', 'dancers', 'stream'],\n",
    "             ['buildings', 'coinsinfountain', 'manfishing', 'sailing2','dancers', 'monarch'],\n",
    "             ['plane', 'monarch', 'sailing3', 'carnivaldolls', 'lighthouse', 'womanhat'],\n",
    "             ['coinsinfountain', 'caps', 'monarch', 'house', 'ocean', 'churchandcapitol']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loading extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (requires steps 1 and 2 to be completed)\n",
    "data = pd.read_pickle(\"./features.pkl\")\n",
    "data = data.loc[:,~data.columns.str.contains('Unnamed')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training on LIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All predictions will be save in results.\n",
    "# This makes it easy to evaluate correlations on different subsets later on.\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for tqdm(split in range(10)): # random splits\n",
    "    for model in tqdm(sorted(data.codebook.unique()), leave=False): # codebook models\n",
    "        \n",
    "        # Create dir to save trained svr model in\n",
    "        log_dir = \"./regression_models\"\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        # Select data\n",
    "        idcs = (data.codebook == model) & \\\n",
    "               (data.dataset == \"liveiqa\")\n",
    "\n",
    "        # Split data (there are predefined splits)\n",
    "        data_train = pd.DataFrame(columns=data.columns.tolist() + [\"modus\", \"split\", \"preds\"])\n",
    "        data_train = data_train.append(data.loc[idcs & (~data.refname.isin(val_names[split]))])\n",
    "                \n",
    "        data_val = pd.DataFrame(columns=data.columns.tolist() + [\"modus\", \"split\", \"preds\"])\n",
    "        data_val = data_val.append(data.loc[idcs & (data.refname.isin(val_names[split]))])\n",
    "\n",
    "        # Get features\n",
    "        betas_train = np.vstack(data_train.beta.values)\n",
    "        betas_val = np.vstack(data_val.beta.values)\n",
    "\n",
    "        # On training data, find parameters to scale features to the range [-1, 1]\n",
    "        scaler = MinMaxScaler(feature_range=[-1,1])\n",
    "        scaler.fit(betas_train)\n",
    "        dump(scaler, os.path.join(log_dir, \"minmaxscaler_{}_{}.joblib\".format(model, split)))\n",
    "        \n",
    "        # Apply parameters to train and test data\n",
    "        betas_train = scaler.transform(betas_train)\n",
    "        betas_val = scaler.transform(betas_val)\n",
    "\n",
    "        # Fit and save support vector machine\n",
    "        svr = svm.NuSVR(kernel='linear', C=1.0, nu=0.5, cache_size=1000)\n",
    "        svr.fit(betas_train, data_train.q_norm)\n",
    "        dump(svr, os.path.join(log_dir, \"svr_{}_{}.joblib\".format(model, split)))\n",
    "        \n",
    "        # Save results on training set\n",
    "        data_train.loc[:, \"modus\"] = \"train\"\n",
    "        data_train.loc[:, \"split\"] = split\n",
    "        data_train.loc[:, \"preds\"] = svr.predict(betas_train)\n",
    "\n",
    "        # Save results on test set\n",
    "        data_val.loc[:, \"modus\"] = \"test\"\n",
    "        data_val.loc[:, \"split\"] = split\n",
    "        data_val.loc[:, \"preds\"] = svr.predict(betas_val)            \n",
    "\n",
    "        # Save results in dataFrame\n",
    "        results = results.append(data_train, ignore_index=True)\n",
    "        results = results.append(data_val, ignore_index=True)\n",
    "        \n",
    "results.to_pickle(\"./predictions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Cross-database evaluation on TID2013 and CSIQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid_names_unique = [\"i01\", \"i02\", \"i07\", \"i12\", \"i15\", \"i25\"]\n",
    "\n",
    "data.loc[data.refname == \"i03\", \"refname\"] = \"caps\"\n",
    "data.loc[data.refname == \"i04\", \"refname\"] = \"womanhat\"\n",
    "data.loc[data.refname == \"i05\", \"refname\"] = \"bikes\"\n",
    "data.loc[data.refname == \"i06\", \"refname\"] = \"sailing1\"\n",
    "data.loc[data.refname == \"i08\", \"refname\"] = \"buildings\"\n",
    "\n",
    "data.loc[data.refname == \"i09\", \"refname\"] = \"sailing2\"\n",
    "data.loc[data.refname == \"i10\", \"refname\"] = \"sailing3\"\n",
    "data.loc[data.refname == \"i11\", \"refname\"] = \"sailing4\"\n",
    "data.loc[data.refname == \"i13\", \"refname\"] = \"stream\"\n",
    "data.loc[data.refname == \"i14\", \"refname\"] = \"rapids\"\n",
    "\n",
    "data.loc[data.refname == \"i16\", \"refname\"] = \"ocean\"\n",
    "data.loc[data.refname == \"i17\", \"refname\"] = \"statue\"\n",
    "data.loc[data.refname == \"i18\", \"refname\"] = \"woman\"\n",
    "data.loc[data.refname == \"i19\", \"refname\"] = \"lighthouse\"\n",
    "data.loc[data.refname == \"i20\", \"refname\"] = \"plane\"\n",
    "\n",
    "data.loc[data.refname == \"i21\", \"refname\"] = \"lighthouse2\"\n",
    "data.loc[data.refname == \"i22\", \"refname\"] = \"house\"\n",
    "data.loc[data.refname == \"i23\", \"refname\"] = \"parrots\"\n",
    "data.loc[data.refname == \"i24\", \"refname\"] = \"paintedhouse\"\n",
    "\n",
    "data.loc[data.distortion == \"wn\", \"distortion\"] = \"awgn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in tqdm(range(10)): # random splits\n",
    "    for model in tqdm(sorted(data.codebook.unique()), leave=False): # codebook models\n",
    "        \n",
    "        # Select data\n",
    "        idcs = (data.codebook == model) & \\\n",
    "               (data.dataset == \"tid2013\")\n",
    "        \n",
    "        # Create dataFrame for tid\n",
    "        data_tid = pd.DataFrame(columns=data.columns.tolist() + [\"modus\", \"split\", \"preds\"])\n",
    "        # Avoid content spill - only use reference images not contained in training set\n",
    "        data_tid = data_tid.append(data.loc[idcs & (data.refname.isin(val_names[split] + tid_names_unique))])\n",
    "        \n",
    "        # Select data\n",
    "        idcs = (data.codebook == model) & \\\n",
    "               (data.dataset == \"csiq\")\n",
    "        \n",
    "        # Create dataFrame for csiq\n",
    "        data_csiq = pd.DataFrame(columns=data.columns.tolist() + [\"modus\", \"split\", \"preds\"])\n",
    "        # We can use all image as LIVE and CSIQ do not share any reference images\n",
    "        data_csiq = data_csiq.append(data.loc[idcs])\n",
    "\n",
    "        # Get features\n",
    "        betas_tid = np.vstack(data_tid.beta.values)\n",
    "        betas_csiq = np.vstack(data_csiq.beta.values)\n",
    "        \n",
    "        scaler = load(os.path.join(log_dir, \"minmaxscaler_{}_{}.joblib\".format(model, split)))\n",
    "        \n",
    "        # Apply parameters to test data\n",
    "        betas_tid = scaler.transform(betas_tid)\n",
    "        betas_csiq = scaler.transform(betas_csiq)\n",
    "\n",
    "        svr = load(os.path.join(log_dir, \"svr_{}_{}.joblib\".format(model, split)))\n",
    "        \n",
    "        # Save results on tid test set\n",
    "        data_tid.loc[:, \"modus\"] = \"test\"\n",
    "        data_tid.loc[:, \"split\"] = split\n",
    "        data_tid.loc[:, \"preds\"] = svr.predict(betas_tid)            \n",
    "        \n",
    "        # Save results on csiq test set\n",
    "        data_csiq.loc[:, \"modus\"] = \"test\"\n",
    "        data_csiq.loc[:, \"split\"] = split\n",
    "        data_csiq.loc[:, \"preds\"] = svr.predict(betas_csiq)            \n",
    "\n",
    "        # Save results in dataFrame\n",
    "        results = results.append(data_tid, ignore_index=True)\n",
    "        results = results.append(data_csiq, ignore_index=True)\n",
    "        \n",
    "        # Compute correlation - this is only a early on sanity check to see if everything is working\n",
    "        # Actual evaluation is done below\n",
    "        pcc_tid = pearsonr(data_tid.loc[:, \"preds\"], data_tid.q_norm)[0]\n",
    "        pcc_csiq = pearsonr(data_csiq.loc[:, \"preds\"], data_csiq.q_norm)[0]\n",
    "        \n",
    "results.to_pickle(\"./predictions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Computation of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results.distortion == \"wn\", \"distortion\"] = \"awgn\"\n",
    "\n",
    "# Setting up the correlation tables\n",
    "corr_columns = [\"pc_full\", \"sc_full\",\n",
    "                \"pc_jpeg\", \"sc_jpeg\",\n",
    "                \"pc_jp2k\", \"sc_jp2k\",\n",
    "                \"pc_gblur\", \"sc_gblur\",\n",
    "                \"pc_awgn\", \"sc_awgn\",\n",
    "                \"pc_shared\", \"sc_shared\"]\n",
    "\n",
    "correlations = pd.DataFrame(columns=[\"model\", \"dataset\"] + corr_columns)\n",
    "\n",
    "# Distortion types considered in the paper\n",
    "dists = [\"full\", \"jpeg\", \"jp2k\", \"gblur\", \"awgn\", \"shared\"]\n",
    "\n",
    "for db in tqdm(results.dataset.unique()):\n",
    "    for codebook in tqdm([\"cornia\", \"patches\", \"laplace\", \"normal\", \"uniform\"], leave=False):\n",
    "        for dist in tqdm(dists, leave=False):\n",
    "            pccs, sroccs = [], []\n",
    "            for split in results.split.unique():\n",
    "\n",
    "                if dist == \"full\":\n",
    "                    _dists = results.loc[results.dataset == db].distortion.unique()\n",
    "                elif dist == \"shared\":\n",
    "                    _dists = [\"jpeg\", \"jp2k\", \"gblur\", \"awgn\"]\n",
    "                else:\n",
    "                    _dists = [dist]\n",
    "                \n",
    "                \n",
    "                # Select predictions of this split\n",
    "                idcs = (results.codebook == codebook) & \\\n",
    "                       (results.dataset == db) & \\\n",
    "                       (results.split == split) & \\\n",
    "                       (results.modus == \"test\") & \\\n",
    "                       (results.distortion.isin(_dists))\n",
    "\n",
    "                if not np.any(idcs): \n",
    "                    continue\n",
    "\n",
    "                # Compute correlations between quality predictions and quality annotations\n",
    "                pccs.append(pearsonr(results.loc[idcs].preds, results.loc[idcs].q_norm)[0])\n",
    "                sroccs.append(spearmanr(results.loc[idcs].preds, results.loc[idcs].q_norm)[0])\n",
    "            \n",
    "            # Save correlations\n",
    "            row_idx = (correlations.dataset == db) & (correlations.model == codebook)\n",
    "                      \n",
    "            if not np.any(row_idx):\n",
    "                row_idx = correlations.shape[0]\n",
    "                \n",
    "            correlations.loc[row_idx, \"dataset\"] = db\n",
    "            correlations.loc[row_idx, \"model\"] = codebook\n",
    "            correlations.loc[row_idx, \"pc_{}\".format(dist)] = np.mean(pccs)\n",
    "            correlations.loc[row_idx, \"sc_{}\".format(dist)] = np.mean(sroccs)\n",
    "            \n",
    "correlations[corr_columns] = correlations[corr_columns].apply(pd.to_numeric)\n",
    "correlations.to_pickle(\"correlations.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Print results tables (Tables 1 and 2 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations.loc[correlations.dataset == \"liveiqa\"].round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations.loc[correlations.dataset == \"tid2013\"].round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations.loc[correlations.dataset == \"csiq\"].round(decimals=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
